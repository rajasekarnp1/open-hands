import pytest
from unittest.mock import MagicMock, AsyncMock

from src.core.router import ProviderRouter
from src.models import (
    ChatCompletionRequest,
    ChatMessage,
    ProviderConfig,
    ModelInfo,
    RateLimit,
    ProviderType,
    ModelCapability,
    ProviderStatus,
    RoutingRule
)

pytestmark = pytest.mark.asyncio

# --- Fixtures ---

@pytest.fixture
def mock_provider_configs() -> dict[str, ProviderConfig]:
    return {
        "provider_fast": ProviderConfig(
            name="provider_fast",
            display_name="Provider Fast",
            provider_type=ProviderType.FREE,
            base_url="http://fast.api",
            models=[
                ModelInfo(name="fast-model", display_name="Fast Model", provider="provider_fast", capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.CODE_GENERATION], context_length=8000, is_free=True),
                ModelInfo(name="fast-haiku-like", display_name="Fast Haiku", provider="provider_fast", capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.SUMMARIZATION], context_length=200000, is_free=True)
            ],
            rate_limit=RateLimit(requests_per_minute=100),
            priority=1,
            status=ProviderStatus.ACTIVE,
            supports_streaming=True
        ),
        "provider_quality": ProviderConfig(
            name="provider_quality",
            display_name="Provider Quality",
            provider_type=ProviderType.PAID, # Assume paid implies quality
            base_url="http://quality.api",
            models=[
                ModelInfo(name="quality-opus-like", display_name="Quality Opus", provider="provider_quality", capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.ADVANCED_REASONING, ModelCapability.CODE_GENERATION], context_length=200000, is_free=False),
                ModelInfo(name="quality-sonnet-like", display_name="Quality Sonnet", provider="provider_quality", capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.REASONING], context_length=128000, is_free=False)
            ],
            rate_limit=RateLimit(requests_per_minute=50),
            priority=2,
            status=ProviderStatus.ACTIVE,
            supports_streaming=True
        ),
        "provider_balanced": ProviderConfig(
            name="provider_balanced",
            display_name="Provider Balanced",
            provider_type=ProviderType.FREE,
            base_url="http://balanced.api",
            models=[
                ModelInfo(name="balanced-model", display_name="Balanced Model", provider="provider_balanced", capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.REASONING, ModelCapability.CODE_GENERATION], context_length=32000, is_free=True)
            ],
            rate_limit=RateLimit(requests_per_minute=70),
            priority=3,
            status=ProviderStatus.ACTIVE,
            supports_streaming=True
        ),
        "provider_inactive": ProviderConfig(
            name="provider_inactive",
            display_name="Provider Inactive",
            provider_type=ProviderType.FREE,
            base_url="http://inactive.api",
            models=[ModelInfo(name="inactive-model", display_name="Inactive Model", provider="provider_inactive", capabilities=[ModelCapability.TEXT_GENERATION], context_length=8000, is_free=True)],
            rate_limit=RateLimit(requests_per_minute=10),
            priority=4,
            status=ProviderStatus.INACTIVE, # This provider is inactive
            supports_streaming=True
        ),
    }

@pytest.fixture
def router(mock_provider_configs: dict[str, ProviderConfig]) -> ProviderRouter:
    # For these tests, we'll use default routing rules generated by ProviderRouter's __init__
    # and then add the mock_provider_configs to it.
    # In a real scenario, routing_rules might also come from config.
    return ProviderRouter(providers=mock_provider_configs, routing_rules=[])


# --- Test Cases ---

async def test_route_with_requested_provider(router: ProviderRouter):
    request = ChatCompletionRequest(
        messages=[ChatMessage(role="user", content="Hello")],
        provider="provider_quality" # Request a specific provider
    )
    provider_chain = await router.get_provider_chain(request)
    assert provider_chain == ["provider_quality"]

async def test_route_with_requested_provider_unavailable(router: ProviderRouter, mock_provider_configs):
    # Make provider_quality temporarily unavailable by changing its status in the router's copy
    router.providers["provider_quality"].status = ProviderStatus.ERROR

    request = ChatCompletionRequest(
        messages=[ChatMessage(role="user", content="Hello")],
        provider="provider_quality" # Request a specific provider that is now unavailable
    )
    # Since the requested provider is unavailable, it should NOT be returned.
    # The router should then proceed with normal routing logic.
    provider_chain = await router.get_provider_chain(request)
    assert "provider_quality" not in provider_chain
    assert len(provider_chain) > 0 # Should fallback to other providers

    # Reset status for other tests
    router.providers["provider_quality"].status = ProviderStatus.ACTIVE


async def test_route_with_model_quality_fastest(router: ProviderRouter):
    request = ChatCompletionRequest(
        messages=[ChatMessage(role="user", content="Summarize this quickly")],
        model_quality="fastest"
    )
    provider_chain = await router.get_provider_chain(request)
    # provider_fast should be prioritized due to its name and "fastest" heuristic in router
    # and "fast-haiku-like" model.
    # The exact order depends on scoring, but provider_fast should be high.
    assert provider_chain[0] == "provider_fast"

async def test_route_with_model_quality_best(router: ProviderRouter):
    request = ChatCompletionRequest(
        messages=[ChatMessage(role="user", content="Deep analysis needed")],
        model_quality="best_quality"
    )
    provider_chain = await router.get_provider_chain(request)
    # provider_quality should be prioritized due to its models and "best_quality" heuristic
    assert provider_chain[0] == "provider_quality"

async def test_route_with_model_quality_balanced(router: ProviderRouter):
    request = ChatCompletionRequest(
        messages=[ChatMessage(role="user", content="Balanced approach please")],
        model_quality="balanced"
    )
    provider_chain = await router.get_provider_chain(request)
    # provider_balanced or provider_quality (Sonnet) might appear high.
    # This test is less deterministic without exact score comparison, but ensures it runs.
    assert len(provider_chain) > 0
    # Example: provider_quality (Sonnet) or provider_balanced might be preferred.
    # For now, just check that provider_fast (purely speed-focused) is not the first.
    if provider_chain: # if any providers returned
      assert provider_chain[0] != "provider_fast" # Assuming quality/balanced model is preferred over pure speed here


async def test_route_with_coding_keywords(router: ProviderRouter, mock_provider_configs):
    # Ensure provider_fast's fast-model and provider_quality's quality-opus-like have CODE_GENERATION
    # This is set in the fixture.

    request = ChatCompletionRequest(
        messages=[ChatMessage(role="user", content="Write a python function to sort a list.")],
        # No model_quality specified, rely on content-based capability inference
    )
    provider_chain = await router.get_provider_chain(request)
    # Providers with CODE_GENERATION capability should be scored higher.
    # provider_fast (fast-model) and provider_quality (quality-opus-like) both have it.
    # provider_balanced also has it.
    # The order will depend on other scoring factors (priority, free status, etc.)
    # For this test, let's ensure that providers offering code generation are present.

    found_coding_provider = False
    for provider_name in provider_chain:
        provider_conf = mock_provider_configs[provider_name]
        if any(ModelCapability.CODE_GENERATION in model.capabilities for model in provider_conf.models):
            found_coding_provider = True
            break
    assert found_coding_provider, "No provider with code generation capability was selected"
    # A more specific assertion could be:
    # assert provider_chain[0] in ["provider_fast", "provider_quality", "provider_balanced"]


async def test_route_fallback_when_preferred_unavailable(router: ProviderRouter, mock_provider_configs):
    # Make provider_fast (highest priority by default) unavailable
    router.providers["provider_fast"].status = ProviderStatus.ERROR

    request = ChatCompletionRequest(messages=[ChatMessage(role="user", content="Generic request")])
    provider_chain = await router.get_provider_chain(request)

    assert "provider_fast" not in provider_chain # Should be skipped
    assert len(provider_chain) > 0 # Should have fallbacks
    if provider_chain: # if there are fallbacks
        # provider_quality is next in priority (2) vs provider_balanced (3)
        assert provider_chain[0] == "provider_quality"

    # Reset status for other tests
    router.providers["provider_fast"].status = ProviderStatus.ACTIVE


async def test_route_all_relevant_providers_inactive(router: ProviderRouter):
    # Make all active providers inactive for this test
    original_statuses = {}
    for name, conf in router.providers.items():
        original_statuses[name] = conf.status
        if name != "provider_inactive": # provider_inactive is already inactive
            conf.status = ProviderStatus.INACTIVE

    request = ChatCompletionRequest(messages=[ChatMessage(role="user", content="Hello")])
    provider_chain = await router.get_provider_chain(request)
    assert len(provider_chain) == 0 # No active providers should be found

    # Restore statuses
    for name, status in original_statuses.items():
        router.providers[name].status = status

async def test_route_with_specific_rule_match(router: ProviderRouter):
    # Add a specific rule that should be matched
    test_rule = RoutingRule(
        name="specific_image_rule",
        conditions={"content_keywords": ["image", "picture"], "capabilities": [ModelCapability.VISION]},
        provider_preferences=["provider_quality"], # Assuming provider_quality could handle vision
        fallback_chain=["provider_fast"],
        is_active=True
    )
    # Temporarily modify provider_quality to have a vision model for this test
    original_model = router.providers["provider_quality"].models[0]
    vision_model = ModelInfo(name="vision-model", display_name="Vision Model", provider="provider_quality",
                             capabilities=[ModelCapability.VISION, ModelCapability.TEXT_GENERATION],
                             context_length=4096, is_free=False)
    router.providers["provider_quality"].models.append(vision_model)

    router.add_routing_rule(test_rule)

    request = ChatCompletionRequest(messages=[ChatMessage(role="user", content="Describe this image.")])
    provider_chain = await router.get_provider_chain(request)

    assert provider_chain[0] == "provider_quality"

    # Clean up: remove rule and restore model
    router.remove_routing_rule("specific_image_rule")
    router.providers["provider_quality"].models = [m for m in router.providers["provider_quality"].models if m.name != "vision-model"]


async def test_default_provider_order(router: ProviderRouter):
    # Test when no rules match and no specific requests are made
    request = ChatCompletionRequest(messages=[ChatMessage(role="user", content="A very generic request.")])

    # Temporarily clear specific routing rules to ensure default order is tested based on priority
    original_rules = router.routing_rules
    router.routing_rules = []

    provider_chain = await router.get_provider_chain(request)

    # Expected order based on priority in mock_provider_configs (active ones)
    # provider_fast (1), provider_quality (2), provider_balanced (3)
    expected_order = ["provider_fast", "provider_quality", "provider_balanced"]

    active_returned_providers = [p for p in provider_chain if router.providers[p].status == ProviderStatus.ACTIVE]

    # The router's _score_providers method adds other bonuses (free, streaming, model count)
    # so the order might not be strictly by priority alone.
    # This test checks that the highest priority provider is considered first if all else is equal.
    # For this test, it's enough to see that they are returned and provider_fast is likely first.
    assert len(active_returned_providers) == 3
    if active_returned_providers:
        assert active_returned_providers[0] == "provider_fast" # provider_fast has high base score + free bonus

    router.routing_rules = original_rules # Restore rules
```
